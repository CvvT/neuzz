{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "import socket\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argvv = ['./test']\n",
    "seed_list = glob.glob('./neuzz_in/*')\n",
    "seed_list.sort()\n",
    "SPLIT_RATIO = len(seed_list)\n",
    "rand_index = np.arange(SPLIT_RATIO)\n",
    "np.random.shuffle(seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./neuzz_in/id:000041,src:000040,op:int8,pos:17,val:+0,+cov\n",
      "./neuzz_in/id:000003,src:000000,op:int8,pos:0,val:+0,+cov\n",
      "./neuzz_in/id:000019,src:000017,op:arith8,pos:6,val:+13,+cov\n",
      "./neuzz_in/id:000099,src:000097,op:flip2,pos:46,+cov\n",
      "./neuzz_in/id:000124,src:000121,op:int8,pos:58,val:+0,+cov\n",
      "./neuzz_in/id:000067,src:000065,op:flip2,pos:30,+cov\n",
      "./neuzz_in/id:000082,src:000080,op:int8,pos:37,val:+0,+cov\n",
      "./neuzz_in/id:000013,src:000012,op:flip1,pos:3,+cov\n",
      "./neuzz_in/id:000045,src:000043,op:arith8,pos:19,val:+21,+cov\n",
      "./neuzz_in/id:000070,src:000067,op:int8,pos:31,val:+0,+cov\n",
      "./neuzz_in/id:000072,src:000069,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000061,src:000059,op:flip4,pos:27,+cov\n",
      "./neuzz_in/id:000069,src:000067,op:flip4,pos:31,+cov\n",
      "./neuzz_in/id:000062,src:000059,op:int8,pos:27,val:+0,+cov\n",
      "./neuzz_in/id:000011,src:000010,op:flip1,pos:2,+cov\n",
      "./neuzz_in/id:000133,src:000131,op:arith8,pos:63,val:+11,+cov\n",
      "./neuzz_in/id:000064,src:000062,op:havoc,rep:8,+cov\n",
      "./neuzz_in/id:000015,src:000013,op:havoc,rep:8,+cov\n",
      "./neuzz_in/id:000113,src:000111,op:arith8,pos:53,val:-5,+cov\n",
      "./neuzz_in/id:000021,src:000020,op:flip1,pos:7,+cov\n",
      "./neuzz_in/id:000066,src:000064,op:int8,pos:29,val:+0,+cov\n",
      "./neuzz_in/id:000111,src:000109,op:arith8,pos:52,val:-20,+cov\n",
      "./neuzz_in/id:000005,src:000000,op:havoc,rep:64\n",
      "./neuzz_in/id:000089,src:000088,op:arith8,pos:41,val:-19,+cov\n",
      "./neuzz_in/id:000026,src:000024,op:int8,pos:9,val:+0,+cov\n",
      "./neuzz_in/id:000095,src:000094,op:arith8,pos:44,val:+9,+cov\n",
      "./neuzz_in/id:000117,src:000115,op:arith8,pos:55,val:+13,+cov\n",
      "./neuzz_in/id:000052,src:000049,op:int8,pos:22,val:+0,+cov\n",
      "./neuzz_in/id:000090,src:000088,op:int8,pos:41,val:+0,+cov\n",
      "./neuzz_in/id:000084,src:000081,op:int8,pos:38,val:+0,+cov\n",
      "./neuzz_in/id:000057,src:000056,op:arith8,pos:25,val:+13,+cov\n",
      "./neuzz_in/id:000118,src:000115,op:int8,pos:55,val:+0,+cov\n",
      "./neuzz_in/id:000087,src:000085,op:flip4,pos:40,+cov\n",
      "./neuzz_in/id:000024,src:000023,op:arith8,pos:8,val:-7,+cov\n",
      "./neuzz_in/id:000043,src:000042,op:flip1,pos:18,+cov\n",
      "./neuzz_in/id:000060,src:000057,op:int8,pos:26,val:+0,+cov\n",
      "./neuzz_in/id:000031,src:000029,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000017,src:000016,op:flip4,pos:5,+cov\n",
      "./neuzz_in/id:000121,src:000119,op:arith8,pos:57,val:-14,+cov\n",
      "./neuzz_in/id:000042,src:000040,op:havoc,rep:4,+cov\n",
      "./neuzz_in/id:000132,src:000130,op:int8,pos:62,val:+0,+cov\n",
      "./neuzz_in/id:000047,src:000044+000045,op:splice,rep:4,+cov\n",
      "./neuzz_in/id:000096,src:000094,op:int8,pos:44,val:+0,+cov\n",
      "./neuzz_in/id:000002,src:000000,op:flip4,pos:5\n",
      "./neuzz_in/id:000009,src:000000,op:havoc,rep:64,+cov\n",
      "./neuzz_in/id:000116,src:000113,op:int8,pos:54,val:+0,+cov\n",
      "./neuzz_in/id:000131,src:000130,op:arith8,pos:62,val:-9,+cov\n",
      "./neuzz_in/id:000123,src:000121,op:arith8,pos:58,val:+15,+cov\n",
      "./neuzz_in/id:000093,src:000092,op:flip4,pos:43,+cov\n",
      "./neuzz_in/id:000027,src:000025,op:flip1,pos:10,+cov\n",
      "./neuzz_in/id:000125,src:000123,op:arith8,pos:59,val:-30,+cov\n",
      "./neuzz_in/id:000006,src:000000,op:havoc,rep:128\n",
      "./neuzz_in/id:000119,src:000117,op:flip2,pos:56,+cov\n",
      "./neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\n",
      "./neuzz_in/id:000086,src:000083,op:int8,pos:39,val:+0,+cov\n",
      "./neuzz_in/id:000108,src:000105,op:int8,pos:50,val:+0,+cov\n",
      "./neuzz_in/id:000012,src:000011,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000110,src:000107,op:int8,pos:51,val:+0,+cov\n",
      "./neuzz_in/id:000020,src:000017+000009,op:splice,rep:2,+cov\n",
      "./neuzz_in/id:000122,src:000119,op:int8,pos:57,val:+0,+cov\n",
      "./neuzz_in/id:000010,src:000009,op:arith8,pos:1,val:-33,+cov\n",
      "./neuzz_in/id:000114,src:000111,op:int8,pos:53,val:+0,+cov\n",
      "./neuzz_in/id:000102,src:000099,op:int8,pos:47,val:+0,+cov\n",
      "./neuzz_in/id:000100,src:000097,op:int8,pos:46,val:+0,+cov\n",
      "./neuzz_in/id:000065,src:000064,op:arith8,pos:29,val:+21,+cov\n",
      "./neuzz_in/id:000038,src:000035,op:flip4,pos:15,+cov\n",
      "./neuzz_in/id:000120,src:000117,op:int8,pos:56,val:+0,+cov\n",
      "./neuzz_in/id:000097,src:000095,op:flip2,pos:45,+cov\n",
      "./neuzz_in/id:000077,src:000075,op:arith8,pos:35,val:+10,+cov\n",
      "./neuzz_in/id:000036,src:000033,op:int8,pos:14,val:+0,+cov\n",
      "./neuzz_in/id:000074,src:000071+000057,op:splice,rep:2,+cov\n",
      "./neuzz_in/id:000126,src:000123,op:int8,pos:59,val:+0,+cov\n",
      "./neuzz_in/id:000051,src:000049,op:arith8,pos:22,val:+13,+cov\n",
      "./neuzz_in/id:000094,src:000092,op:arith8,pos:43,val:-21,+cov\n",
      "./neuzz_in/id:000101,src:000099,op:arith8,pos:47,val:-10,+cov\n",
      "./neuzz_in/id:000054,src:000051,op:int8,pos:23,val:+0,+cov\n",
      "./neuzz_in/id:000128,src:000125,op:int8,pos:60,val:+0,+cov\n",
      "./neuzz_in/id:000107,src:000105,op:flip2,pos:50,+cov\n",
      "./neuzz_in/id:000033,src:000032,op:arith8,pos:13,val:-2,+cov\n",
      "./neuzz_in/id:000007,src:000000,op:havoc,rep:4\n",
      "./neuzz_in/id:000079,src:000077,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000127,src:000125,op:flip2,pos:60,+cov\n",
      "./neuzz_in/id:000000,orig:test\n",
      "./neuzz_in/id:000076,src:000074,op:int8,pos:34,val:+0,+cov\n",
      "./neuzz_in/id:000004,src:000000,op:int8,pos:120,val:+0\n",
      "./neuzz_in/id:000104,src:000101,op:int8,pos:48,val:+0,+cov\n",
      "./neuzz_in/id:000092,src:000089,op:arith8,pos:42,val:+18,+cov\n",
      "./neuzz_in/id:000030,src:000027,op:int8,pos:11,val:+0,+cov\n",
      "./neuzz_in/id:000035,src:000033,op:arith8,pos:14,val:+3,+cov\n",
      "./neuzz_in/id:000098,src:000095,op:int8,pos:45,val:+0,+cov\n",
      "./neuzz_in/id:000025,src:000024,op:flip1,pos:9,+cov\n",
      "./neuzz_in/id:000115,src:000113,op:arith8,pos:54,val:-7,+cov\n",
      "./neuzz_in/id:000130,src:000127,op:arith8,pos:61,val:-2,+cov\n",
      "./neuzz_in/id:000078,src:000075,op:int8,pos:35,val:+0,+cov\n",
      "./neuzz_in/id:000073,src:000071,op:arith8,pos:33,val:+27,+cov\n",
      "./neuzz_in/id:000058,src:000056,op:int8,pos:25,val:+0,+cov\n",
      "./neuzz_in/id:000018,src:000016,op:int8,pos:5,val:+0,+cov\n",
      "./neuzz_in/id:000029,src:000027,op:arith8,pos:11,val:+15,+cov\n",
      "./neuzz_in/id:000083,src:000081,op:arith8,pos:38,val:+20,+cov\n",
      "./neuzz_in/id:000049,src:000048,op:arith8,pos:21,val:+21,+cov\n",
      "./neuzz_in/id:000001,src:000000,op:flip2,pos:9\n",
      "./neuzz_in/id:000068,src:000065,op:int8,pos:30,val:+0,+cov\n",
      "./neuzz_in/id:000008,src:000000,op:havoc,rep:64,+cov\n",
      "./neuzz_in/id:000112,src:000109,op:int8,pos:52,val:+0,+cov\n",
      "./neuzz_in/id:000032,src:000031,op:arith8,pos:12,val:+11,+cov\n",
      "./neuzz_in/id:000071,src:000069,op:flip4,pos:32,+cov\n",
      "./neuzz_in/id:000088,src:000085,op:flip4,pos:40,+cov\n",
      "./neuzz_in/id:000091,src:000088,op:int16,pos:41,val:+100,+cov\n",
      "./neuzz_in/id:000109,src:000107,op:arith8,pos:51,val:+21,+cov\n",
      "./neuzz_in/id:000063,src:000061,op:arith8,pos:28,val:+30,+cov\n",
      "./neuzz_in/id:000081,src:000080,op:arith8,pos:37,val:-14,+cov\n",
      "./neuzz_in/id:000080,src:000079,op:arith8,pos:36,val:+7,+cov\n",
      "./neuzz_in/id:000044,src:000042,op:int8,pos:18,val:+0,+cov\n",
      "./neuzz_in/id:000048,src:000045,op:arith8,pos:20,val:+23,+cov\n",
      "./neuzz_in/id:000059,src:000057,op:flip2,pos:26,+cov\n",
      "./neuzz_in/id:000046,src:000043,op:int8,pos:19,val:+0,+cov\n",
      "./neuzz_in/id:000075,src:000074,op:arith8,pos:34,val:+14,+cov\n",
      "./neuzz_in/id:000037,src:000033,op:int16,pos:14,val:+100,+cov\n",
      "./neuzz_in/id:000106,src:000103,op:int8,pos:49,val:+0,+cov\n",
      "./neuzz_in/id:000040,src:000039,op:arith8,pos:16,val:-12,+cov\n",
      "./neuzz_in/id:000016,src:000015,op:arith8,pos:4,val:-12,+cov\n",
      "./neuzz_in/id:000023,src:000022,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000014,src:000013,op:arith8,pos:3,val:-15,+cov\n",
      "./neuzz_in/id:000039,src:000035,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000022,src:000020,op:arith8,pos:7,val:-7,+cov\n",
      "./neuzz_in/id:000055,src:000053,op:havoc,rep:4,+cov\n",
      "./neuzz_in/id:000053,src:000051,op:flip2,pos:23,+cov\n",
      "./neuzz_in/id:000050,src:000048,op:int8,pos:21,val:+0,+cov\n",
      "./neuzz_in/id:000028,src:000025,op:int8,pos:10,val:+0,+cov\n",
      "./neuzz_in/id:000034,src:000032,op:int8,pos:13,val:+0,+cov\n",
      "./neuzz_in/id:000129,src:000125,op:int16,pos:60,val:+100,+cov\n",
      "./neuzz_in/id:000105,src:000103,op:arith8,pos:49,val:-22,+cov\n",
      "./neuzz_in/id:000085,src:000083,op:flip2,pos:39,+cov\n",
      "./neuzz_in/id:000103,src:000101,op:arith8,pos:48,val:-13,+cov\n",
      "./neuzz_in/id:000056,src:000053,op:havoc,rep:4,+cov\n"
     ]
    }
   ],
   "source": [
    "MAX_FILE_SIZE = 512\n",
    "call=subprocess.check_output\n",
    "raw_bitmap = {}\n",
    "tmp_cnt = []\n",
    "out = ''\n",
    "for f in seed_list:\n",
    "    tmp_list = []\n",
    "    try:\n",
    "        # append \"-o tmp_file\" to strip's arguments to avoid tampering tested binary.\n",
    "        print(f)\n",
    "        with open(f) as myinput:\n",
    "            out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512'] + argvv, stdin=myinput)\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"find a crash\")\n",
    "    for line in out.splitlines():\n",
    "        edge = line.split(':')[0]\n",
    "        tmp_cnt.append(edge)\n",
    "        tmp_list.append(edge)\n",
    "    raw_bitmap[f] = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(tmp_cnt).most_common()\n",
    "label = [int(f[0]) for f in counter]\n",
    "bitmap = np.zeros((len(seed_list), len(label)))\n",
    "for idx,i in enumerate(seed_list):\n",
    "    tmp = raw_bitmap[i]\n",
    "    for j in tmp:\n",
    "        if int(j) in label:\n",
    "            bitmap[idx][label.index((int(j)))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./bitmaps/\") == False:\n",
    "    os.makedirs('./bitmaps')\n",
    "fit_bitmap = np.unique(bitmap,axis=1)\n",
    "MAX_BITMAP_SIZE = fit_bitmap.shape[1]\n",
    "for idx,i in enumerate(seed_list):\n",
    "    file_name = \"./bitmaps/\"+i.split('/')[-1]\n",
    "    np.save(file_name,fit_bitmap[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_label = []\n",
    "for i in range(fit_bitmap.shape[1]):\n",
    "    edges = []\n",
    "    for j in range(bitmap.shape[1]):\n",
    "        if (bitmap[:, j] == fit_bitmap[:, i]).all():\n",
    "            edges.append(j)\n",
    "    fit_label.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(edge):\n",
    "    index = label.index(edge)\n",
    "    for i, edges in enumerate(fit_label):\n",
    "        if index in edges:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accur_1(y_true,y_pred):\n",
    "    y_true = tf.round(y_true)\n",
    "    pred = tf.round(y_pred)\n",
    "    summ = tf.constant(MAX_BITMAP_SIZE,dtype=tf.float32)\n",
    "    wrong_num = tf.subtract(summ,tf.reduce_sum(tf.cast(tf.equal(y_true, pred),tf.float32),axis=-1))\n",
    "    right_1_num = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true,tf.bool), tf.cast(pred,tf.bool)),tf.float32),axis=-1)\n",
    "    ret = K.mean(tf.divide(right_1_num,tf.add(right_1_num,wrong_num)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    batch_size = 32\n",
    "    num_classes = MAX_BITMAP_SIZE\n",
    "    epochs = 50\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4096, input_dim=MAX_FILE_SIZE))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    opt = keras.optimizers.adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[accur_1])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 258)               1057026   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 258)               0         \n",
      "=================================================================\n",
      "Total params: 3,158,274\n",
      "Trainable params: 3,158,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(lb,ub):\n",
    "    seed = np.zeros((ub-lb,MAX_FILE_SIZE))\n",
    "    bitmap = np.zeros((ub-lb,MAX_BITMAP_SIZE))\n",
    "    for i in range(lb,ub):\n",
    "        tmp = open(seed_list[i],'r').read()\n",
    "        ln = len(tmp)\n",
    "        if ln < MAX_FILE_SIZE:\n",
    "            tmp = tmp + (MAX_FILE_SIZE - ln) * '\\0'\n",
    "        seed[i-lb] = [ord(j) for j in list(tmp)]\n",
    "    for i in range(lb,ub):\n",
    "        file_name = \"./bitmaps/\"+ seed_list[i].split('/')[-1] + \".npy\"\n",
    "        bitmap[i-lb] = np.load(file_name)\n",
    "    return seed,bitmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(batch_size):\n",
    "    global seed_list\n",
    "    while 1:\n",
    "        np.random.shuffle(seed_list)\n",
    "        for i in range(0,SPLIT_RATIO,batch_size):\n",
    "            if (i+batch_size) > SPLIT_RATIO:\n",
    "                x,y=generate_training_data(i,SPLIT_RATIO)\n",
    "                x = x.astype('float32')/255\n",
    "            else:\n",
    "                x,y=generate_training_data(i,i+batch_size)\n",
    "                x = x.astype('float32')/255\n",
    "            yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.7\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    loss_history = LossHistory()\n",
    "    lrate = keras.callbacks.LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [loss_history, lrate]\n",
    "    model.fit_generator(train_generate(8),\n",
    "              steps_per_epoch = (SPLIT_RATIO/8 + 1),\n",
    "              epochs=100,\n",
    "              verbose=1, callbacks=callbacks_list)\n",
    "    # Save model and weights\n",
    "    model.save_weights(\"hard_label.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.3834 - accur_1: 0.6040\n",
      "0.001\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.2103 - accur_1: 0.7160\n",
      "0.001\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.1604 - accur_1: 0.7571\n",
      "0.001\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.1419 - accur_1: 0.7618\n",
      "0.001\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.1311 - accur_1: 0.7802\n",
      "0.001\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.1257 - accur_1: 0.7904\n",
      "0.001\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.1245 - accur_1: 0.7850\n",
      "0.001\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.1165 - accur_1: 0.8014\n",
      "0.001\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.1124 - accur_1: 0.8094\n",
      "0.0007\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.1070 - accur_1: 0.8067\n",
      "0.0007\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.1023 - accur_1: 0.8215\n",
      "0.0007\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.1033 - accur_1: 0.8183\n",
      "0.0007\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0981 - accur_1: 0.8261\n",
      "0.0007\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.1000 - accur_1: 0.8285\n",
      "0.0007\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0994 - accur_1: 0.8244\n",
      "0.0007\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0990 - accur_1: 0.8255\n",
      "0.0007\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0949 - accur_1: 0.8283\n",
      "0.0007\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0923 - accur_1: 0.8328\n",
      "0.0007\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0907 - accur_1: 0.8380\n",
      "0.00049\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0858 - accur_1: 0.8401\n",
      "0.00049\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0845 - accur_1: 0.8459\n",
      "0.00049\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0833 - accur_1: 0.8454\n",
      "0.00049\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0861 - accur_1: 0.8394\n",
      "0.00049\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0824 - accur_1: 0.8418\n",
      "0.00049\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0816 - accur_1: 0.8463\n",
      "0.00049\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0807 - accur_1: 0.8472\n",
      "0.00049\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0785 - accur_1: 0.8527\n",
      "0.00049\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0789 - accur_1: 0.8543\n",
      "0.00049\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0757 - accur_1: 0.8605\n",
      "0.000343\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0762 - accur_1: 0.8531\n",
      "0.000343\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0733 - accur_1: 0.8646\n",
      "0.000343\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0738 - accur_1: 0.8604\n",
      "0.000343\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0721 - accur_1: 0.8650\n",
      "0.000343\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0720 - accur_1: 0.8624\n",
      "0.000343\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0715 - accur_1: 0.8666\n",
      "0.000343\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0710 - accur_1: 0.8668\n",
      "0.000343\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0710 - accur_1: 0.8634\n",
      "0.000343\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0696 - accur_1: 0.8643\n",
      "0.000343\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0706 - accur_1: 0.8635\n",
      "0.0002401\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0686 - accur_1: 0.8644\n",
      "0.0002401\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0672 - accur_1: 0.8699\n",
      "0.0002401\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0675 - accur_1: 0.8653\n",
      "0.0002401\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0666 - accur_1: 0.8737\n",
      "0.0002401\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0658 - accur_1: 0.8785\n",
      "0.0002401\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0657 - accur_1: 0.8705\n",
      "0.0002401\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0670 - accur_1: 0.8677\n",
      "0.0002401\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0676 - accur_1: 0.8774\n",
      "0.0002401\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0649 - accur_1: 0.8742\n",
      "0.0002401\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 1s 29ms/step - loss: 0.0656 - accur_1: 0.8703\n",
      "0.00016807\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0637 - accur_1: 0.8771\n",
      "0.00016807\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0632 - accur_1: 0.8835\n",
      "0.00016807\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0624 - accur_1: 0.8858\n",
      "0.00016807\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0632 - accur_1: 0.8810\n",
      "0.00016807\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0628 - accur_1: 0.8813\n",
      "0.00016807\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0620 - accur_1: 0.8794\n",
      "0.00016807\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0620 - accur_1: 0.8875\n",
      "0.00016807\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0615 - accur_1: 0.8863\n",
      "0.00016807\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0612 - accur_1: 0.8892\n",
      "0.00016807\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0607 - accur_1: 0.8877\n",
      "0.000117649\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0607 - accur_1: 0.8881\n",
      "0.000117649\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0597 - accur_1: 0.8896\n",
      "0.000117649\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0597 - accur_1: 0.8909\n",
      "0.000117649\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0597 - accur_1: 0.8895\n",
      "0.000117649\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0594 - accur_1: 0.8924\n",
      "0.000117649\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0592 - accur_1: 0.8895\n",
      "0.000117649\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0589 - accur_1: 0.8923\n",
      "0.000117649\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0585 - accur_1: 0.8925\n",
      "0.000117649\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0585 - accur_1: 0.8898\n",
      "0.000117649\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0586 - accur_1: 0.8933\n",
      "8.23543e-05\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0581 - accur_1: 0.8944\n",
      "8.23543e-05\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0581 - accur_1: 0.8933\n",
      "8.23543e-05\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0578 - accur_1: 0.8919\n",
      "8.23543e-05\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0577 - accur_1: 0.8946\n",
      "8.23543e-05\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0574 - accur_1: 0.8952\n",
      "8.23543e-05\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0578 - accur_1: 0.8916\n",
      "8.23543e-05\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0571 - accur_1: 0.8953\n",
      "8.23543e-05\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0575 - accur_1: 0.8927\n",
      "8.23543e-05\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0573 - accur_1: 0.8944\n",
      "8.23543e-05\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0571 - accur_1: 0.8941\n",
      "5.764801e-05\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0568 - accur_1: 0.8937\n",
      "5.764801e-05\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0568 - accur_1: 0.8953\n",
      "5.764801e-05\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0568 - accur_1: 0.8976\n",
      "5.764801e-05\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0566 - accur_1: 0.8962\n",
      "5.764801e-05\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0569 - accur_1: 0.8960\n",
      "5.764801e-05\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0567 - accur_1: 0.8945\n",
      "5.764801e-05\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0563 - accur_1: 0.8960\n",
      "5.764801e-05\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0559 - accur_1: 0.8987\n",
      "5.764801e-05\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0562 - accur_1: 0.8971\n",
      "5.764801e-05\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0562 - accur_1: 0.8975\n",
      "4.0353607e-05\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0560 - accur_1: 0.8980\n",
      "4.0353607e-05\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.0555 - accur_1: 0.8989\n",
      "4.0353607e-05\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0556 - accur_1: 0.8986\n",
      "4.0353607e-05\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0556 - accur_1: 0.8980\n",
      "4.0353607e-05\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0555 - accur_1: 0.8978\n",
      "4.0353607e-05\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0553 - accur_1: 0.8982\n",
      "4.0353607e-05\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0552 - accur_1: 0.8982\n",
      "4.0353607e-05\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0554 - accur_1: 0.8985\n",
      "4.0353607e-05\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.0555 - accur_1: 0.8975\n",
      "4.0353607e-05\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0554 - accur_1: 0.8991\n",
      "2.82475249e-05\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0551 - accur_1: 0.8987\n",
      "2.82475249e-05\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_file(fl, isfile, vectorize=True):\n",
    "    seed = np.zeros((1,MAX_FILE_SIZE))\n",
    "    if isfile:\n",
    "        tmp = open(fl,'r').read()\n",
    "    else:\n",
    "        tmp = fl\n",
    "    ln = len(tmp)\n",
    "    if ln < MAX_FILE_SIZE:\n",
    "        tmp = tmp + (MAX_FILE_SIZE - ln) * '\\0'\n",
    "    seed[0] = [ord(j) for j in list(tmp)]\n",
    "    if vectorize:\n",
    "        seed = seed.astype('float32')/255\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(model, edge, seed, isfile, vectorize=True):\n",
    "    layer_list = [(layer.name, layer) for layer in model.layers]\n",
    "    index = getIndex(edge)\n",
    "    loss = layer_list[-2][1].output[:,index]\n",
    "    grads = K.gradients(loss,model.input)[0]\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    x=vectorize_file(seed, isfile, vectorize)\n",
    "    loss_value, grads_value = iterate([x])\n",
    "    idx = np.flip(np.argsort(np.absolute(grads_value),axis=1)[:, -MAX_FILE_SIZE:].reshape((MAX_FILE_SIZE,)),0)\n",
    "    val = np.sign(grads_value[0][idx])\n",
    "    return idx, val, grads_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  9,  8,  7,  0, 19, 33, 11, 34, 15, 30, 58, 28, 35,  5, 31,  6,\n",
       "       32, 60, 16, 20, 56,  1,  4, 14, 61, 39, 21, 38, 12, 23, 55])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 137, \"adsdsdsdsdsdsd\", False, True)\n",
    "idx[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  8,  0,  7,  9, 19, 33, 34, 11, 58, 15, 30, 28, 35,  5, 31, 16,\n",
       "        1, 60, 20, 56, 32,  6, 61, 21, 39,  4, 55, 14, 38, 18, 23])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 137, \"adsdsdsdsdsdsd\", False, False)\n",
    "idx[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "32582 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-38d0e029c79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32582\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ccdf21a2e96a>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(model, edge, seed, isfile, vectorize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlayer_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-42af34b11501>\u001b[0m in \u001b[0;36mgetIndex\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 32582 is not in list"
     ]
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 32582, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", True, False)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.8555671)\n",
      "(1, -0.8356758)\n",
      "(2, 0.83039975)\n",
      "(3, -0.83026075)\n",
      "(4, -0.8201449)\n",
      "(5, 0.801137)\n",
      "(6, -0.670687)\n",
      "(7, 0.66335636)\n",
      "(8, -0.6429825)\n",
      "(9, 0.599087)\n",
      "(10, -0.5970227)\n",
      "(11, -0.5074947)\n",
      "(12, -0.41564494)\n",
      "(13, 0.4065544)\n",
      "(14, 0.38793638)\n",
      "(15, -0.3855853)\n",
      "(16, -0.37170208)\n",
      "(17, -0.3676876)\n",
      "(18, 0.36227956)\n",
      "(19, -0.3375323)\n",
      "(20, -0.3321437)\n",
      "(21, 0.31335232)\n",
      "(22, 0.31173545)\n",
      "(23, -0.30742148)\n",
      "(24, 0.30016378)\n",
      "(25, 0.2920049)\n",
      "(26, -0.29166275)\n",
      "(27, -0.29134077)\n",
      "(28, -0.2884982)\n",
      "(29, 0.2704623)\n",
      "(30, -0.26894107)\n",
      "(31, -0.26489347)\n",
      "(32, -0.24957502)\n",
      "(33, -0.21879551)\n",
      "(34, -0.21444349)\n",
      "(35, 0.20216048)\n",
      "(36, -0.19997865)\n",
      "(37, 0.19455038)\n",
      "(38, -0.1934537)\n",
      "(39, 0.18425728)\n",
      "(40, 0.18185082)\n",
      "(41, -0.17984045)\n",
      "(42, 0.16503325)\n",
      "(43, -0.15825674)\n",
      "(44, -0.15661451)\n",
      "(45, -0.15237948)\n",
      "(46, -0.14327422)\n",
      "(47, -0.1413459)\n",
      "(48, -0.13360952)\n",
      "(49, 0.11023159)\n",
      "(50, -0.07967442)\n",
      "(51, 0.07360465)\n",
      "(52, 0.06726114)\n",
      "(53, -0.066510595)\n",
      "(54, -0.06634131)\n",
      "(55, 0.06600034)\n",
      "(56, -0.06400159)\n",
      "(57, 0.060154717)\n",
      "(58, 0.058344524)\n",
      "(59, 0.058061328)\n",
      "(60, -0.05720707)\n",
      "(61, 0.05670479)\n",
      "(62, 0.056201883)\n",
      "(63, -0.054625217)\n",
      "(64, -0.05089661)\n",
      "(65, -0.04992505)\n",
      "(66, 0.04949507)\n",
      "(67, 0.049134005)\n",
      "(68, -0.048743628)\n",
      "(69, -0.048634283)\n",
      "(70, -0.048404373)\n",
      "(71, 0.0477214)\n",
      "(72, 0.046846926)\n",
      "(73, -0.046253122)\n",
      "(74, 0.04544089)\n",
      "(75, 0.045386452)\n",
      "(76, 0.04526514)\n",
      "(77, -0.045087233)\n",
      "(78, -0.043905824)\n",
      "(79, 0.04372906)\n",
      "(80, 0.04332457)\n",
      "(81, -0.04297752)\n",
      "(82, 0.042852063)\n",
      "(83, 0.04264283)\n",
      "(84, -0.042511567)\n",
      "(85, 0.0421982)\n",
      "(86, 0.041772097)\n",
      "(87, 0.0416079)\n",
      "(88, 0.04114344)\n",
      "(89, 0.041075394)\n",
      "(90, -0.040790286)\n",
      "(91, -0.040283572)\n",
      "(92, -0.04018516)\n",
      "(93, -0.04017233)\n",
      "(94, 0.040075667)\n",
      "(95, 0.039449397)\n",
      "(96, -0.03915765)\n",
      "(97, 0.039022297)\n",
      "(98, 0.038836412)\n",
      "(99, -0.038798258)\n",
      "(100, -0.038452998)\n",
      "(101, 0.038173564)\n",
      "(102, 0.03760043)\n",
      "(103, 0.037102416)\n",
      "(104, -0.036973838)\n",
      "(105, 0.036633514)\n",
      "(106, -0.036571518)\n",
      "(107, 0.03607175)\n",
      "(108, -0.035703357)\n",
      "(109, 0.03558793)\n",
      "(110, -0.035470627)\n",
      "(111, -0.03538527)\n",
      "(112, -0.03534176)\n",
      "(113, 0.0352975)\n",
      "(114, 0.0351638)\n",
      "(115, 0.035081122)\n",
      "(116, 0.03499875)\n",
      "(117, -0.034987584)\n",
      "(118, -0.034917824)\n",
      "(119, 0.034856312)\n",
      "(120, 0.034630693)\n",
      "(121, -0.03452547)\n",
      "(122, -0.03451738)\n",
      "(123, 0.034485232)\n",
      "(124, -0.034453914)\n",
      "(125, -0.034419786)\n",
      "(126, -0.03421797)\n",
      "(127, 0.03416998)\n",
      "(128, -0.033415817)\n",
      "(129, 0.03337352)\n",
      "(130, 0.03333343)\n",
      "(131, 0.03320044)\n",
      "(132, -0.033106193)\n",
      "(133, -0.03305857)\n",
      "(134, -0.032991674)\n",
      "(135, -0.032852866)\n",
      "(136, -0.032798607)\n",
      "(137, 0.03278836)\n",
      "(138, -0.032592963)\n",
      "(139, -0.03256756)\n",
      "(140, 0.03247655)\n",
      "(141, -0.032448824)\n",
      "(142, 0.03226196)\n",
      "(143, -0.032218553)\n",
      "(144, 0.03207306)\n",
      "(145, 0.032040663)\n",
      "(146, 0.031958155)\n",
      "(147, 0.031481877)\n",
      "(148, 0.031443037)\n",
      "(149, 0.031225383)\n",
      "(150, 0.030677862)\n",
      "(151, 0.030628398)\n",
      "(152, -0.030623727)\n",
      "(153, -0.030493299)\n",
      "(154, -0.030368544)\n",
      "(155, -0.030226607)\n",
      "(156, -0.030174546)\n",
      "(157, 0.030091308)\n",
      "(158, 0.029901287)\n",
      "(159, 0.029899377)\n",
      "(160, 0.0298968)\n",
      "(161, 0.02973369)\n",
      "(162, 0.029453369)\n",
      "(163, -0.029438226)\n",
      "(164, 0.02926232)\n",
      "(165, 0.029247934)\n",
      "(166, -0.029193724)\n",
      "(167, 0.029102806)\n",
      "(168, -0.029010018)\n",
      "(169, 0.028850619)\n",
      "(170, 0.028822282)\n",
      "(171, 0.028689064)\n",
      "(172, -0.028621532)\n",
      "(173, -0.028452788)\n",
      "(174, -0.028430987)\n",
      "(175, 0.028203346)\n",
      "(176, -0.027898021)\n",
      "(177, -0.02788904)\n",
      "(178, 0.027834775)\n",
      "(179, 0.027280215)\n",
      "(180, 0.0272746)\n",
      "(181, -0.02706106)\n",
      "(182, 0.027033336)\n",
      "(183, 0.026727617)\n",
      "(184, 0.02670015)\n",
      "(185, 0.026692402)\n",
      "(186, -0.02668295)\n",
      "(187, -0.026475482)\n",
      "(188, -0.026303872)\n",
      "(189, -0.026241537)\n",
      "(190, -0.025932422)\n",
      "(191, 0.025865404)\n",
      "(192, 0.025823437)\n",
      "(193, 0.02564725)\n",
      "(194, 0.025599122)\n",
      "(195, 0.025356878)\n",
      "(196, 0.025163962)\n",
      "(197, -0.025037145)\n",
      "(198, 0.024844756)\n",
      "(199, 0.024791004)\n",
      "(200, -0.024770372)\n",
      "(201, 0.024762752)\n",
      "(202, -0.024699327)\n",
      "(203, -0.024595104)\n",
      "(204, -0.0243016)\n",
      "(205, 0.023912754)\n",
      "(206, -0.023721483)\n",
      "(207, 0.023546016)\n",
      "(208, 0.023262134)\n",
      "(209, 0.023162758)\n",
      "(210, -0.023126269)\n",
      "(211, -0.023108628)\n",
      "(212, 0.02300806)\n",
      "(213, -0.022927957)\n",
      "(214, -0.02288519)\n",
      "(215, 0.022511927)\n",
      "(216, -0.02239159)\n",
      "(217, -0.022389684)\n",
      "(218, -0.022258358)\n",
      "(219, -0.021964721)\n",
      "(220, 0.021910537)\n",
      "(221, 0.021897906)\n",
      "(222, -0.021883782)\n",
      "(223, 0.021837685)\n",
      "(224, 0.02178979)\n",
      "(225, -0.021715246)\n",
      "(226, 0.021700075)\n",
      "(227, 0.021628361)\n",
      "(228, 0.021585539)\n",
      "(229, -0.021556312)\n",
      "(230, -0.021524422)\n",
      "(231, -0.021489933)\n",
      "(232, -0.021476535)\n",
      "(233, 0.021318428)\n",
      "(234, -0.021305969)\n",
      "(235, 0.021180429)\n",
      "(236, -0.02107355)\n",
      "(237, -0.021011788)\n",
      "(238, -0.020941678)\n",
      "(239, -0.020911925)\n",
      "(240, 0.020808036)\n",
      "(241, 0.02076704)\n",
      "(242, -0.020510957)\n",
      "(243, 0.020365419)\n",
      "(244, -0.020339303)\n",
      "(245, -0.020320464)\n",
      "(246, -0.02021186)\n",
      "(247, -0.02013801)\n",
      "(248, -0.01991431)\n",
      "(249, 0.01965598)\n",
      "(250, 0.019642737)\n",
      "(251, -0.019634958)\n",
      "(252, -0.019614335)\n",
      "(253, 0.019418627)\n",
      "(254, -0.019373551)\n",
      "(255, -0.019337319)\n",
      "(256, 0.019325294)\n",
      "(257, 0.019281734)\n",
      "(258, -0.019154605)\n",
      "(259, 0.01914455)\n",
      "(260, 0.019110367)\n",
      "(261, 0.019091623)\n",
      "(262, -0.019079112)\n",
      "(263, -0.01900186)\n",
      "(264, 0.018996391)\n",
      "(265, -0.018944146)\n",
      "(266, 0.01884073)\n",
      "(267, 0.018828077)\n",
      "(268, 0.018801875)\n",
      "(269, 0.01863948)\n",
      "(270, -0.018540604)\n",
      "(271, 0.018485399)\n",
      "(272, -0.018378213)\n",
      "(273, -0.018324886)\n",
      "(274, -0.018243304)\n",
      "(275, -0.018221704)\n",
      "(276, 0.018219963)\n",
      "(277, 0.018012995)\n",
      "(278, -0.017978638)\n",
      "(279, 0.017959803)\n",
      "(280, 0.017951783)\n",
      "(281, 0.017863099)\n",
      "(282, 0.01765989)\n",
      "(283, 0.01715759)\n",
      "(284, -0.01708597)\n",
      "(285, -0.017059615)\n",
      "(286, 0.016946927)\n",
      "(287, 0.016750105)\n",
      "(288, -0.016585182)\n",
      "(289, -0.016520854)\n",
      "(290, -0.01651083)\n",
      "(291, 0.016484767)\n",
      "(292, -0.016467638)\n",
      "(293, -0.016395688)\n",
      "(294, 0.016378388)\n",
      "(295, 0.01635)\n",
      "(296, 0.016288165)\n",
      "(297, 0.01603142)\n",
      "(298, 0.0158859)\n",
      "(299, -0.015396658)\n",
      "(300, 0.015277166)\n",
      "(301, 0.01526197)\n",
      "(302, 0.015247293)\n",
      "(303, 0.0151814325)\n",
      "(304, 0.014960403)\n",
      "(305, 0.014724963)\n",
      "(306, 0.014691729)\n",
      "(307, -0.014484675)\n",
      "(308, -0.014342738)\n",
      "(309, -0.014199784)\n",
      "(310, -0.01406915)\n",
      "(311, 0.0139411185)\n",
      "(312, 0.013892511)\n",
      "(313, 0.013751019)\n",
      "(314, 0.013560162)\n",
      "(315, 0.013537388)\n",
      "(316, -0.013489078)\n",
      "(317, -0.013347875)\n",
      "(318, 0.0132808)\n",
      "(319, -0.013276419)\n",
      "(320, -0.01324062)\n",
      "(321, 0.013223693)\n",
      "(322, 0.013208821)\n",
      "(323, 0.013191927)\n",
      "(324, -0.012918366)\n",
      "(325, -0.012903722)\n",
      "(326, -0.012664245)\n",
      "(327, -0.012658011)\n",
      "(328, 0.012581914)\n",
      "(329, -0.012550954)\n",
      "(330, -0.012535855)\n",
      "(331, -0.012517113)\n",
      "(332, 0.012472027)\n",
      "(333, 0.012453824)\n",
      "(334, 0.0123825185)\n",
      "(335, -0.012174438)\n",
      "(336, 0.012173023)\n",
      "(337, -0.012065845)\n",
      "(338, -0.012013393)\n",
      "(339, -0.012007735)\n",
      "(340, -0.011965566)\n",
      "(341, -0.011882057)\n",
      "(342, 0.011866221)\n",
      "(343, 0.011842764)\n",
      "(344, -0.011826336)\n",
      "(345, -0.011794714)\n",
      "(346, -0.011787303)\n",
      "(347, -0.0117492415)\n",
      "(348, -0.011553049)\n",
      "(349, 0.011392855)\n",
      "(350, -0.011382837)\n",
      "(351, 0.011212378)\n",
      "(352, 0.011170347)\n",
      "(353, -0.011168482)\n",
      "(354, -0.01108858)\n",
      "(355, -0.01102142)\n",
      "(356, 0.010948072)\n",
      "(357, 0.010905003)\n",
      "(358, -0.010880616)\n",
      "(359, 0.010859419)\n",
      "(360, 0.010778844)\n",
      "(361, -0.010734851)\n",
      "(362, -0.010726434)\n",
      "(363, 0.010428802)\n",
      "(364, -0.0104094)\n",
      "(365, -0.010394291)\n",
      "(366, -0.010391511)\n",
      "(367, 0.010314915)\n",
      "(368, -0.010270538)\n",
      "(369, -0.010211987)\n",
      "(370, -0.010197947)\n",
      "(371, 0.010176949)\n",
      "(372, 0.0101573635)\n",
      "(373, -0.010051182)\n",
      "(374, -0.010019635)\n",
      "(375, 0.009963602)\n",
      "(376, -0.009912867)\n",
      "(377, -0.009844259)\n",
      "(378, -0.009824212)\n",
      "(379, 0.00980269)\n",
      "(380, 0.009783974)\n",
      "(381, 0.009735899)\n",
      "(382, -0.009656574)\n",
      "(383, 0.009362772)\n",
      "(384, -0.009320867)\n",
      "(385, -0.009298044)\n",
      "(386, 0.009246229)\n",
      "(387, -0.009050771)\n",
      "(388, -0.008907274)\n",
      "(389, -0.0088727195)\n",
      "(390, -0.008812331)\n",
      "(391, 0.008808913)\n",
      "(392, -0.008703278)\n",
      "(393, -0.008687569)\n",
      "(394, 0.008643398)\n",
      "(395, -0.00860608)\n",
      "(396, -0.00857665)\n",
      "(397, 0.008562969)\n",
      "(398, -0.008358723)\n",
      "(399, -0.008289143)\n",
      "(400, 0.008286327)\n",
      "(401, -0.008180677)\n",
      "(402, -0.008145388)\n",
      "(403, -0.008069169)\n",
      "(404, 0.008049492)\n",
      "(405, -0.007981752)\n",
      "(406, -0.007871661)\n",
      "(407, 0.007854472)\n",
      "(408, -0.0077908803)\n",
      "(409, -0.0077905897)\n",
      "(410, 0.007784889)\n",
      "(411, -0.007661814)\n",
      "(412, -0.0076179863)\n",
      "(413, 0.0075028455)\n",
      "(414, -0.0074704494)\n",
      "(415, 0.007446212)\n",
      "(416, -0.0074226633)\n",
      "(417, 0.0073875217)\n",
      "(418, 0.00733273)\n",
      "(419, -0.007269065)\n",
      "(420, 0.0070664193)\n",
      "(421, 0.0069431886)\n",
      "(422, -0.0068104267)\n",
      "(423, 0.006751962)\n",
      "(424, 0.0066222297)\n",
      "(425, 0.0062863263)\n",
      "(426, 0.006136531)\n",
      "(427, -0.00599299)\n",
      "(428, 0.005970021)\n",
      "(429, -0.0059185093)\n",
      "(430, -0.0057346085)\n",
      "(431, 0.00571404)\n",
      "(432, -0.005671347)\n",
      "(433, -0.00563494)\n",
      "(434, 0.00558736)\n",
      "(435, -0.0054717055)\n",
      "(436, 0.0054161493)\n",
      "(437, 0.0054057287)\n",
      "(438, 0.0054017175)\n",
      "(439, 0.005384635)\n",
      "(440, -0.005300468)\n",
      "(441, 0.0052647395)\n",
      "(442, -0.0051462017)\n",
      "(443, 0.004989839)\n",
      "(444, -0.0049771164)\n",
      "(445, -0.0049441513)\n",
      "(446, -0.004915351)\n",
      "(447, -0.004871374)\n",
      "(448, 0.0048129186)\n",
      "(449, 0.0046468787)\n",
      "(450, 0.0044107544)\n",
      "(451, -0.0043833405)\n",
      "(452, -0.0042493762)\n",
      "(453, 0.003967663)\n",
      "(454, -0.003911568)\n",
      "(455, 0.0039109057)\n",
      "(456, 0.003868742)\n",
      "(457, -0.003733974)\n",
      "(458, -0.003703216)\n",
      "(459, -0.0036891075)\n",
      "(460, -0.0035953657)\n",
      "(461, 0.0034836205)\n",
      "(462, -0.0033087656)\n",
      "(463, 0.0032484164)\n",
      "(464, -0.0032116212)\n",
      "(465, 0.0030536763)\n",
      "(466, 0.003043281)\n",
      "(467, -0.0029988491)\n",
      "(468, 0.0029945495)\n",
      "(469, -0.0028279722)\n",
      "(470, -0.0027829763)\n",
      "(471, -0.0027392926)\n",
      "(472, 0.0027334145)\n",
      "(473, -0.002640347)\n",
      "(474, 0.0025113008)\n",
      "(475, -0.0024669736)\n",
      "(476, 0.0024144454)\n",
      "(477, -0.0022963677)\n",
      "(478, -0.0022356184)\n",
      "(479, 0.0022195745)\n",
      "(480, -0.002126825)\n",
      "(481, -0.0020422488)\n",
      "(482, -0.0020263698)\n",
      "(483, 0.0019427724)\n",
      "(484, -0.0018905848)\n",
      "(485, -0.0018734778)\n",
      "(486, 0.0018527396)\n",
      "(487, -0.0018118657)\n",
      "(488, 0.0017491714)\n",
      "(489, 0.0017383788)\n",
      "(490, 0.0016062092)\n",
      "(491, -0.0015983493)\n",
      "(492, -0.0014924034)\n",
      "(493, 0.0014405232)\n",
      "(494, 0.0014280379)\n",
      "(495, 0.0013728808)\n",
      "(496, -0.0012682555)\n",
      "(497, -0.0012451457)\n",
      "(498, -0.0010944847)\n",
      "(499, 0.0010689986)\n",
      "(500, 0.00088679465)\n",
      "(501, 0.0008520596)\n",
      "(502, 0.00073884334)\n",
      "(503, 0.0005689338)\n",
      "(504, 0.0005613249)\n",
      "(505, -0.00048796833)\n",
      "(506, 0.00048380718)\n",
      "(507, -0.00045196898)\n",
      "(508, 0.0003406834)\n",
      "(509, 0.00029956363)\n",
      "(510, 0.00024804287)\n",
      "(511, 0.00022887718)\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_FILE_SIZE):\n",
    "    print(i, grads.reshape((MAX_FILE_SIZE,))[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 51, 56, 55, 45, 60,  9,  0, 19, 33,  2, 53, 44, 59, 46, 43,  7,\n",
       "       34, 42, 32, 20, 35, 50, 26,  1,  5, 49, 24, 21, 57, 41, 54])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, val = gradient(model, 46697, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", True, False)\n",
    "idx[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, seed, vectorize=False):\n",
    "    x = vectorize_file(seed, True, vectorize)\n",
    "    file_name = file_name = \"./bitmaps/\"+ seed.split('/')[-1] + \".npy\"\n",
    "    return model.predict(x), np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ty = evaluate(model, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.360158e-32,\n",
       "        0.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuzz",
   "language": "python",
   "name": "neuzz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

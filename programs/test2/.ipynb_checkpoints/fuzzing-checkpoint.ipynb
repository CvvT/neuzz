{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "import socket\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argvv = ['./test']\n",
    "seed_list = glob.glob('./neuzz_in/*')\n",
    "seed_list.sort()\n",
    "SPLIT_RATIO = len(seed_list)\n",
    "rand_index = np.arange(SPLIT_RATIO)\n",
    "np.random.shuffle(seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./neuzz_in/id:000041,src:000040,op:int8,pos:17,val:+0,+cov\n",
      "./neuzz_in/id:000003,src:000000,op:int8,pos:0,val:+0,+cov\n",
      "./neuzz_in/id:000019,src:000017,op:arith8,pos:6,val:+13,+cov\n",
      "./neuzz_in/id:000099,src:000097,op:flip2,pos:46,+cov\n",
      "./neuzz_in/id:000124,src:000121,op:int8,pos:58,val:+0,+cov\n",
      "./neuzz_in/id:000067,src:000065,op:flip2,pos:30,+cov\n",
      "./neuzz_in/id:000082,src:000080,op:int8,pos:37,val:+0,+cov\n",
      "./neuzz_in/id:000013,src:000012,op:flip1,pos:3,+cov\n",
      "./neuzz_in/id:000045,src:000043,op:arith8,pos:19,val:+21,+cov\n",
      "./neuzz_in/id:000070,src:000067,op:int8,pos:31,val:+0,+cov\n",
      "./neuzz_in/id:000072,src:000069,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000061,src:000059,op:flip4,pos:27,+cov\n",
      "./neuzz_in/id:000069,src:000067,op:flip4,pos:31,+cov\n",
      "./neuzz_in/id:000062,src:000059,op:int8,pos:27,val:+0,+cov\n",
      "./neuzz_in/id:000011,src:000010,op:flip1,pos:2,+cov\n",
      "./neuzz_in/id:000133,src:000131,op:arith8,pos:63,val:+11,+cov\n",
      "./neuzz_in/id:000064,src:000062,op:havoc,rep:8,+cov\n",
      "./neuzz_in/id:000015,src:000013,op:havoc,rep:8,+cov\n",
      "./neuzz_in/id:000113,src:000111,op:arith8,pos:53,val:-5,+cov\n",
      "./neuzz_in/id:000021,src:000020,op:flip1,pos:7,+cov\n",
      "./neuzz_in/id:000066,src:000064,op:int8,pos:29,val:+0,+cov\n",
      "./neuzz_in/id:000111,src:000109,op:arith8,pos:52,val:-20,+cov\n",
      "./neuzz_in/id:000005,src:000000,op:havoc,rep:64\n",
      "./neuzz_in/id:000089,src:000088,op:arith8,pos:41,val:-19,+cov\n",
      "./neuzz_in/id:000026,src:000024,op:int8,pos:9,val:+0,+cov\n",
      "./neuzz_in/id:000095,src:000094,op:arith8,pos:44,val:+9,+cov\n",
      "./neuzz_in/id:000117,src:000115,op:arith8,pos:55,val:+13,+cov\n",
      "./neuzz_in/id:000052,src:000049,op:int8,pos:22,val:+0,+cov\n",
      "./neuzz_in/id:000090,src:000088,op:int8,pos:41,val:+0,+cov\n",
      "./neuzz_in/id:000084,src:000081,op:int8,pos:38,val:+0,+cov\n",
      "./neuzz_in/id:000057,src:000056,op:arith8,pos:25,val:+13,+cov\n",
      "./neuzz_in/id:000118,src:000115,op:int8,pos:55,val:+0,+cov\n",
      "./neuzz_in/id:000087,src:000085,op:flip4,pos:40,+cov\n",
      "./neuzz_in/id:000024,src:000023,op:arith8,pos:8,val:-7,+cov\n",
      "./neuzz_in/id:000043,src:000042,op:flip1,pos:18,+cov\n",
      "./neuzz_in/id:000060,src:000057,op:int8,pos:26,val:+0,+cov\n",
      "./neuzz_in/id:000031,src:000029,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000017,src:000016,op:flip4,pos:5,+cov\n",
      "./neuzz_in/id:000121,src:000119,op:arith8,pos:57,val:-14,+cov\n",
      "./neuzz_in/id:000042,src:000040,op:havoc,rep:4,+cov\n",
      "./neuzz_in/id:000132,src:000130,op:int8,pos:62,val:+0,+cov\n",
      "./neuzz_in/id:000047,src:000044+000045,op:splice,rep:4,+cov\n",
      "./neuzz_in/id:000096,src:000094,op:int8,pos:44,val:+0,+cov\n",
      "./neuzz_in/id:000002,src:000000,op:flip4,pos:5\n",
      "./neuzz_in/id:000009,src:000000,op:havoc,rep:64,+cov\n",
      "./neuzz_in/id:000116,src:000113,op:int8,pos:54,val:+0,+cov\n",
      "./neuzz_in/id:000131,src:000130,op:arith8,pos:62,val:-9,+cov\n",
      "./neuzz_in/id:000123,src:000121,op:arith8,pos:58,val:+15,+cov\n",
      "./neuzz_in/id:000093,src:000092,op:flip4,pos:43,+cov\n",
      "./neuzz_in/id:000027,src:000025,op:flip1,pos:10,+cov\n",
      "./neuzz_in/id:000125,src:000123,op:arith8,pos:59,val:-30,+cov\n",
      "./neuzz_in/id:000006,src:000000,op:havoc,rep:128\n",
      "./neuzz_in/id:000119,src:000117,op:flip2,pos:56,+cov\n",
      "./neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\n",
      "./neuzz_in/id:000086,src:000083,op:int8,pos:39,val:+0,+cov\n",
      "./neuzz_in/id:000108,src:000105,op:int8,pos:50,val:+0,+cov\n",
      "./neuzz_in/id:000012,src:000011,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000110,src:000107,op:int8,pos:51,val:+0,+cov\n",
      "./neuzz_in/id:000020,src:000017+000009,op:splice,rep:2,+cov\n",
      "./neuzz_in/id:000122,src:000119,op:int8,pos:57,val:+0,+cov\n",
      "./neuzz_in/id:000010,src:000009,op:arith8,pos:1,val:-33,+cov\n",
      "./neuzz_in/id:000114,src:000111,op:int8,pos:53,val:+0,+cov\n",
      "./neuzz_in/id:000102,src:000099,op:int8,pos:47,val:+0,+cov\n",
      "./neuzz_in/id:000100,src:000097,op:int8,pos:46,val:+0,+cov\n",
      "./neuzz_in/id:000065,src:000064,op:arith8,pos:29,val:+21,+cov\n",
      "./neuzz_in/id:000038,src:000035,op:flip4,pos:15,+cov\n",
      "./neuzz_in/id:000120,src:000117,op:int8,pos:56,val:+0,+cov\n",
      "./neuzz_in/id:000097,src:000095,op:flip2,pos:45,+cov\n",
      "./neuzz_in/id:000077,src:000075,op:arith8,pos:35,val:+10,+cov\n",
      "./neuzz_in/id:000036,src:000033,op:int8,pos:14,val:+0,+cov\n",
      "./neuzz_in/id:000074,src:000071+000057,op:splice,rep:2,+cov\n",
      "./neuzz_in/id:000126,src:000123,op:int8,pos:59,val:+0,+cov\n",
      "./neuzz_in/id:000051,src:000049,op:arith8,pos:22,val:+13,+cov\n",
      "./neuzz_in/id:000094,src:000092,op:arith8,pos:43,val:-21,+cov\n",
      "./neuzz_in/id:000101,src:000099,op:arith8,pos:47,val:-10,+cov\n",
      "./neuzz_in/id:000054,src:000051,op:int8,pos:23,val:+0,+cov\n",
      "./neuzz_in/id:000128,src:000125,op:int8,pos:60,val:+0,+cov\n",
      "./neuzz_in/id:000107,src:000105,op:flip2,pos:50,+cov\n",
      "./neuzz_in/id:000033,src:000032,op:arith8,pos:13,val:-2,+cov\n",
      "./neuzz_in/id:000007,src:000000,op:havoc,rep:4\n",
      "./neuzz_in/id:000079,src:000077,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000127,src:000125,op:flip2,pos:60,+cov\n",
      "./neuzz_in/id:000000,orig:test\n",
      "./neuzz_in/id:000076,src:000074,op:int8,pos:34,val:+0,+cov\n",
      "./neuzz_in/id:000004,src:000000,op:int8,pos:120,val:+0\n",
      "./neuzz_in/id:000104,src:000101,op:int8,pos:48,val:+0,+cov\n",
      "./neuzz_in/id:000092,src:000089,op:arith8,pos:42,val:+18,+cov\n",
      "./neuzz_in/id:000030,src:000027,op:int8,pos:11,val:+0,+cov\n",
      "./neuzz_in/id:000035,src:000033,op:arith8,pos:14,val:+3,+cov\n",
      "./neuzz_in/id:000098,src:000095,op:int8,pos:45,val:+0,+cov\n",
      "./neuzz_in/id:000025,src:000024,op:flip1,pos:9,+cov\n",
      "./neuzz_in/id:000115,src:000113,op:arith8,pos:54,val:-7,+cov\n",
      "./neuzz_in/id:000130,src:000127,op:arith8,pos:61,val:-2,+cov\n",
      "./neuzz_in/id:000078,src:000075,op:int8,pos:35,val:+0,+cov\n",
      "./neuzz_in/id:000073,src:000071,op:arith8,pos:33,val:+27,+cov\n",
      "./neuzz_in/id:000058,src:000056,op:int8,pos:25,val:+0,+cov\n",
      "./neuzz_in/id:000018,src:000016,op:int8,pos:5,val:+0,+cov\n",
      "./neuzz_in/id:000029,src:000027,op:arith8,pos:11,val:+15,+cov\n",
      "./neuzz_in/id:000083,src:000081,op:arith8,pos:38,val:+20,+cov\n",
      "./neuzz_in/id:000049,src:000048,op:arith8,pos:21,val:+21,+cov\n",
      "./neuzz_in/id:000001,src:000000,op:flip2,pos:9\n",
      "./neuzz_in/id:000068,src:000065,op:int8,pos:30,val:+0,+cov\n",
      "./neuzz_in/id:000008,src:000000,op:havoc,rep:64,+cov\n",
      "./neuzz_in/id:000112,src:000109,op:int8,pos:52,val:+0,+cov\n",
      "./neuzz_in/id:000032,src:000031,op:arith8,pos:12,val:+11,+cov\n",
      "./neuzz_in/id:000071,src:000069,op:flip4,pos:32,+cov\n",
      "./neuzz_in/id:000088,src:000085,op:flip4,pos:40,+cov\n",
      "./neuzz_in/id:000091,src:000088,op:int16,pos:41,val:+100,+cov\n",
      "./neuzz_in/id:000109,src:000107,op:arith8,pos:51,val:+21,+cov\n",
      "./neuzz_in/id:000063,src:000061,op:arith8,pos:28,val:+30,+cov\n",
      "./neuzz_in/id:000081,src:000080,op:arith8,pos:37,val:-14,+cov\n",
      "./neuzz_in/id:000080,src:000079,op:arith8,pos:36,val:+7,+cov\n",
      "./neuzz_in/id:000044,src:000042,op:int8,pos:18,val:+0,+cov\n",
      "./neuzz_in/id:000048,src:000045,op:arith8,pos:20,val:+23,+cov\n",
      "./neuzz_in/id:000059,src:000057,op:flip2,pos:26,+cov\n",
      "./neuzz_in/id:000046,src:000043,op:int8,pos:19,val:+0,+cov\n",
      "./neuzz_in/id:000075,src:000074,op:arith8,pos:34,val:+14,+cov\n",
      "./neuzz_in/id:000037,src:000033,op:int16,pos:14,val:+100,+cov\n",
      "./neuzz_in/id:000106,src:000103,op:int8,pos:49,val:+0,+cov\n",
      "./neuzz_in/id:000040,src:000039,op:arith8,pos:16,val:-12,+cov\n",
      "./neuzz_in/id:000016,src:000015,op:arith8,pos:4,val:-12,+cov\n",
      "./neuzz_in/id:000023,src:000022,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000014,src:000013,op:arith8,pos:3,val:-15,+cov\n",
      "./neuzz_in/id:000039,src:000035,op:havoc,rep:2,+cov\n",
      "./neuzz_in/id:000022,src:000020,op:arith8,pos:7,val:-7,+cov\n",
      "./neuzz_in/id:000055,src:000053,op:havoc,rep:4,+cov\n",
      "./neuzz_in/id:000053,src:000051,op:flip2,pos:23,+cov\n",
      "./neuzz_in/id:000050,src:000048,op:int8,pos:21,val:+0,+cov\n",
      "./neuzz_in/id:000028,src:000025,op:int8,pos:10,val:+0,+cov\n",
      "./neuzz_in/id:000034,src:000032,op:int8,pos:13,val:+0,+cov\n",
      "./neuzz_in/id:000129,src:000125,op:int16,pos:60,val:+100,+cov\n",
      "./neuzz_in/id:000105,src:000103,op:arith8,pos:49,val:-22,+cov\n",
      "./neuzz_in/id:000085,src:000083,op:flip2,pos:39,+cov\n",
      "./neuzz_in/id:000103,src:000101,op:arith8,pos:48,val:-13,+cov\n",
      "./neuzz_in/id:000056,src:000053,op:havoc,rep:4,+cov\n"
     ]
    }
   ],
   "source": [
    "MAX_FILE_SIZE = 512\n",
    "call=subprocess.check_output\n",
    "raw_bitmap = {}\n",
    "tmp_cnt = []\n",
    "out = ''\n",
    "for f in seed_list:\n",
    "    tmp_list = []\n",
    "    try:\n",
    "        # append \"-o tmp_file\" to strip's arguments to avoid tampering tested binary.\n",
    "        print(f)\n",
    "        with open(f) as myinput:\n",
    "            out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512'] + argvv, stdin=myinput)\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"find a crash\")\n",
    "    for line in out.splitlines():\n",
    "        edge = line.split(':')[0]\n",
    "        tmp_cnt.append(edge)\n",
    "        tmp_list.append(edge)\n",
    "    raw_bitmap[f] = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(tmp_cnt).most_common()\n",
    "label = [int(f[0]) for f in counter]\n",
    "bitmap = np.zeros((len(seed_list), len(label)))\n",
    "for idx,i in enumerate(seed_list):\n",
    "    tmp = raw_bitmap[i]\n",
    "    for j in tmp:\n",
    "        if int(j) in label:\n",
    "            bitmap[idx][label.index((int(j)))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./bitmaps/\") == False:\n",
    "    os.makedirs('./bitmaps')\n",
    "fit_bitmap = np.unique(bitmap,axis=1)\n",
    "MAX_BITMAP_SIZE = fit_bitmap.shape[1]\n",
    "for idx,i in enumerate(seed_list):\n",
    "    file_name = \"./bitmaps/\"+i.split('/')[-1]\n",
    "    np.save(file_name,fit_bitmap[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_label = []\n",
    "for i in range(fit_bitmap.shape[1]):\n",
    "    edges = []\n",
    "    for j in range(bitmap.shape[1]):\n",
    "        if (bitmap[:, j] == fit_bitmap[:, i]).all():\n",
    "            edges.append(j)\n",
    "    fit_label.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(edge):\n",
    "    index = label.index(edge)\n",
    "    for i, edges in enumerate(fit_label):\n",
    "        if index in edges:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accur_1(y_true,y_pred):\n",
    "    y_true = tf.round(y_true)\n",
    "    pred = tf.round(y_pred)\n",
    "    summ = tf.constant(MAX_BITMAP_SIZE,dtype=tf.float32)\n",
    "    wrong_num = tf.subtract(summ,tf.reduce_sum(tf.cast(tf.equal(y_true, pred),tf.float32),axis=-1))\n",
    "    right_1_num = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true,tf.bool), tf.cast(pred,tf.bool)),tf.float32),axis=-1)\n",
    "    ret = K.mean(tf.divide(right_1_num,tf.add(right_1_num,wrong_num)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    batch_size = 32\n",
    "    num_classes = MAX_BITMAP_SIZE\n",
    "    epochs = 50\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4096, input_dim=MAX_FILE_SIZE))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    opt = keras.optimizers.adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[accur_1])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 258)               1057026   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 258)               0         \n",
      "=================================================================\n",
      "Total params: 3,158,274\n",
      "Trainable params: 3,158,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(lb,ub):\n",
    "    seed = np.zeros((ub-lb,MAX_FILE_SIZE))\n",
    "    bitmap = np.zeros((ub-lb,MAX_BITMAP_SIZE))\n",
    "    for i in range(lb,ub):\n",
    "        tmp = open(seed_list[i],'r').read()\n",
    "        ln = len(tmp)\n",
    "        if ln < MAX_FILE_SIZE:\n",
    "            tmp = tmp + (MAX_FILE_SIZE - ln) * '\\0'\n",
    "        seed[i-lb] = [ord(j) for j in list(tmp)]\n",
    "    for i in range(lb,ub):\n",
    "        file_name = \"./bitmaps/\"+ seed_list[i].split('/')[-1] + \".npy\"\n",
    "        bitmap[i-lb] = np.load(file_name)\n",
    "    return seed,bitmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(batch_size):\n",
    "    global seed_list\n",
    "    while 1:\n",
    "        np.random.shuffle(seed_list)\n",
    "        for i in range(0,SPLIT_RATIO,batch_size):\n",
    "            if (i+batch_size) > SPLIT_RATIO:\n",
    "                x,y=generate_training_data(i,SPLIT_RATIO)\n",
    "                x = x.astype('float32')/255\n",
    "            else:\n",
    "                x,y=generate_training_data(i,i+batch_size)\n",
    "                x = x.astype('float32')/255\n",
    "            yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.7\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    loss_history = LossHistory()\n",
    "    lrate = keras.callbacks.LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [loss_history, lrate]\n",
    "    model.fit_generator(train_generate(8),\n",
    "              steps_per_epoch = (SPLIT_RATIO/8 + 1),\n",
    "              epochs=100,\n",
    "              verbose=1, callbacks=callbacks_list)\n",
    "    # Save model and weights\n",
    "    model.save_weights(\"hard_label.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.3900 - accur_1: 0.5973\n",
      "0.001\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.2079 - accur_1: 0.7131\n",
      "0.001\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1619 - accur_1: 0.7604\n",
      "0.001\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.1419 - accur_1: 0.7628\n",
      "0.001\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.1341 - accur_1: 0.7894\n",
      "0.001\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.1277 - accur_1: 0.7837\n",
      "0.001\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.1199 - accur_1: 0.7977\n",
      "0.001\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.1207 - accur_1: 0.7910\n",
      "0.001\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.1110 - accur_1: 0.8130\n",
      "0.0007\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1083 - accur_1: 0.8075\n",
      "0.0007\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1045 - accur_1: 0.8138\n",
      "0.0007\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1013 - accur_1: 0.8188\n",
      "0.0007\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0990 - accur_1: 0.8216\n",
      "0.0007\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0999 - accur_1: 0.8214\n",
      "0.0007\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0951 - accur_1: 0.8303\n",
      "0.0007\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0968 - accur_1: 0.8210\n",
      "0.0007\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0939 - accur_1: 0.8337\n",
      "0.0007\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0900 - accur_1: 0.8354\n",
      "0.0007\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0897 - accur_1: 0.8357\n",
      "0.00049\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0859 - accur_1: 0.8450\n",
      "0.00049\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0851 - accur_1: 0.8409\n",
      "0.00049\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0830 - accur_1: 0.8482\n",
      "0.00049\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0885 - accur_1: 0.8359\n",
      "0.00049\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0839 - accur_1: 0.8422\n",
      "0.00049\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.0804 - accur_1: 0.8532\n",
      "0.00049\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0789 - accur_1: 0.8511\n",
      "0.00049\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0789 - accur_1: 0.8527\n",
      "0.00049\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0785 - accur_1: 0.8523\n",
      "0.00049\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.0775 - accur_1: 0.8541\n",
      "0.000343\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 0.0740 - accur_1: 0.8609\n",
      "0.000343\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 0.0748 - accur_1: 0.8606\n",
      "0.000343\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0741 - accur_1: 0.8591\n",
      "0.000343\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0728 - accur_1: 0.8576\n",
      "0.000343\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0737 - accur_1: 0.8540\n",
      "0.000343\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0709 - accur_1: 0.8625\n",
      "0.000343\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0702 - accur_1: 0.8663\n",
      "0.000343\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0705 - accur_1: 0.8643\n",
      "0.000343\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0700 - accur_1: 0.8655\n",
      "0.000343\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0698 - accur_1: 0.8637\n",
      "0.0002401\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 1s 44ms/step - loss: 0.0678 - accur_1: 0.8711\n",
      "0.0002401\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0669 - accur_1: 0.8728\n",
      "0.0002401\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0674 - accur_1: 0.8690\n",
      "0.0002401\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0673 - accur_1: 0.8664\n",
      "0.0002401\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 0.0662 - accur_1: 0.8723\n",
      "0.0002401\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 0.0659 - accur_1: 0.8696\n",
      "0.0002401\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0648 - accur_1: 0.8812\n",
      "0.0002401\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 0.0645 - accur_1: 0.8703\n",
      "0.0002401\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0646 - accur_1: 0.8742\n",
      "0.0002401\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0646 - accur_1: 0.8821\n",
      "0.00016807\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0640 - accur_1: 0.8743\n",
      "0.00016807\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0643 - accur_1: 0.8792\n",
      "0.00016807\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0628 - accur_1: 0.8725\n",
      "0.00016807\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0619 - accur_1: 0.8768\n",
      "0.00016807\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0617 - accur_1: 0.8885\n",
      "0.00016807\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0615 - accur_1: 0.8847\n",
      "0.00016807\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.0615 - accur_1: 0.8779\n",
      "0.00016807\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0617 - accur_1: 0.8852\n",
      "0.00016807\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.0605 - accur_1: 0.8839\n",
      "0.00016807\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 0.0613 - accur_1: 0.8892\n",
      "0.000117649\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0602 - accur_1: 0.8893\n",
      "0.000117649\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0599 - accur_1: 0.8865\n",
      "0.000117649\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0591 - accur_1: 0.8898\n",
      "0.000117649\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0597 - accur_1: 0.8880\n",
      "0.000117649\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0589 - accur_1: 0.8900\n",
      "0.000117649\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0591 - accur_1: 0.8915\n",
      "0.000117649\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 0.0588 - accur_1: 0.8901\n",
      "0.000117649\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0606 - accur_1: 0.8864\n",
      "0.000117649\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0590 - accur_1: 0.8926\n",
      "0.000117649\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0589 - accur_1: 0.8908\n",
      "8.23543e-05\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0577 - accur_1: 0.8943\n",
      "8.23543e-05\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0578 - accur_1: 0.8956\n",
      "8.23543e-05\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0579 - accur_1: 0.8940\n",
      "8.23543e-05\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0576 - accur_1: 0.8931\n",
      "8.23543e-05\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0579 - accur_1: 0.8917\n",
      "8.23543e-05\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0572 - accur_1: 0.8959\n",
      "8.23543e-05\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 43ms/step - loss: 0.0570 - accur_1: 0.8935\n",
      "8.23543e-05\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.0569 - accur_1: 0.8945\n",
      "8.23543e-05\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0570 - accur_1: 0.8943\n",
      "8.23543e-05\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0567 - accur_1: 0.8947\n",
      "5.764801e-05\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0566 - accur_1: 0.8949\n",
      "5.764801e-05\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.0566 - accur_1: 0.8942\n",
      "5.764801e-05\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0569 - accur_1: 0.8938\n",
      "5.764801e-05\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0568 - accur_1: 0.8963\n",
      "5.764801e-05\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.0564 - accur_1: 0.8955\n",
      "5.764801e-05\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.0562 - accur_1: 0.8961\n",
      "5.764801e-05\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0564 - accur_1: 0.8954\n",
      "5.764801e-05\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.0559 - accur_1: 0.8968\n",
      "5.764801e-05\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0558 - accur_1: 0.8958\n",
      "5.764801e-05\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.0558 - accur_1: 0.8963\n",
      "4.0353607e-05\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.0556 - accur_1: 0.8983\n",
      "4.0353607e-05\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0554 - accur_1: 0.8978\n",
      "4.0353607e-05\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.0556 - accur_1: 0.8975\n",
      "4.0353607e-05\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.0556 - accur_1: 0.8973\n",
      "4.0353607e-05\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.0554 - accur_1: 0.8964\n",
      "4.0353607e-05\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.0552 - accur_1: 0.8961\n",
      "4.0353607e-05\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0551 - accur_1: 0.8970\n",
      "4.0353607e-05\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.0553 - accur_1: 0.8986\n",
      "4.0353607e-05\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.0552 - accur_1: 0.8975\n",
      "4.0353607e-05\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0550 - accur_1: 0.8993\n",
      "2.82475249e-05\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0550 - accur_1: 0.8982\n",
      "2.82475249e-05\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_file(fl, isfile, vectorize=True):\n",
    "    seed = np.zeros((1,MAX_FILE_SIZE))\n",
    "    if isfile:\n",
    "        tmp = open(fl,'r').read()\n",
    "    else:\n",
    "        tmp = fl\n",
    "    ln = len(tmp)\n",
    "    if ln < MAX_FILE_SIZE:\n",
    "        tmp = tmp + (MAX_FILE_SIZE - ln) * '\\0'\n",
    "    seed[0] = [ord(j) for j in list(tmp)]\n",
    "    if vectorize:\n",
    "        seed = seed.astype('float32')/255\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(model, edge, seed, isfile, vectorize=True):\n",
    "    layer_list = [(layer.name, layer) for layer in model.layers]\n",
    "    index = getIndex(edge)\n",
    "    loss = layer_list[-2][1].output[:,index]\n",
    "    grads = K.gradients(loss,model.input)[0]\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    x=vectorize_file(seed, isfile, vectorize)\n",
    "    loss_value, grads_value = iterate([x])\n",
    "    idx = np.flip(np.argsort(np.absolute(grads_value),axis=1)[:, -MAX_FILE_SIZE:].reshape((MAX_FILE_SIZE,)),0)\n",
    "    val = np.sign(grads_value[0][idx])\n",
    "    return idx, val, grads_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 19  7 15 33 30 28 34 14 16  8  2 31 32 35  5 13  3 10 58 18 20  4  6\n",
      " 27 23 39 60 38 56 40 21]\n",
      "(9, 7.1797113)\n",
      "(19, 5.3130484)\n",
      "(7, 4.081481)\n",
      "(15, 3.6039388)\n",
      "(33, 3.3900166)\n",
      "(30, -3.360676)\n",
      "(28, -3.2450254)\n",
      "(34, 3.2305295)\n",
      "(14, 3.1713207)\n",
      "(16, 3.0232043)\n",
      "(8, -3.0090027)\n",
      "(2, 3.0035567)\n",
      "(31, -2.906313)\n",
      "(32, 2.7794313)\n",
      "(35, 2.7166095)\n",
      "(5, 2.4512334)\n",
      "(13, 2.3961582)\n",
      "(3, -2.0681763)\n",
      "(10, 1.8895186)\n",
      "(58, -1.8839817)\n",
      "(18, 1.8257998)\n",
      "(20, 1.7273021)\n",
      "(4, -1.6383826)\n",
      "(6, 1.6315525)\n",
      "(27, -1.5775269)\n",
      "(23, -1.5609598)\n",
      "(39, 1.5214111)\n",
      "(60, -1.5152252)\n",
      "(38, 1.4620535)\n",
      "(56, -1.4037094)\n",
      "(40, 1.3544334)\n",
      "(21, 1.344499)\n"
     ]
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 65010, \"xhqqevmyaua\", False, True)\n",
    "print(idx[:32])\n",
    "for i in range(32):\n",
    "    print(idx[i], grads.reshape((MAX_FILE_SIZE,))[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  9  8  0  7 19 58 33 34 11 60 30 28 35 56 15  1  5 20 61 32 31 38  6\n",
      " 39 16 55 40 21 26  4 63]\n",
      "(2, 4.3056946)\n",
      "(9, 3.4857693)\n",
      "(8, -3.485417)\n",
      "(0, 3.4091678)\n",
      "(7, 3.253026)\n",
      "(19, 3.1068797)\n",
      "(58, -2.4985132)\n",
      "(33, 2.3860934)\n",
      "(34, 2.3333309)\n",
      "(11, -2.0473819)\n",
      "(60, -1.9499453)\n",
      "(30, -1.9294356)\n",
      "(28, -1.8983822)\n",
      "(35, 1.8873602)\n",
      "(56, -1.820421)\n",
      "(15, 1.8177513)\n",
      "(1, 1.6894426)\n",
      "(5, 1.6128211)\n",
      "(20, 1.584897)\n",
      "(61, -1.5840384)\n",
      "(32, 1.5515348)\n",
      "(31, -1.4208286)\n",
      "(38, 1.3686905)\n",
      "(6, 1.3130174)\n",
      "(39, 1.265341)\n",
      "(16, 1.2509533)\n",
      "(55, -1.2221736)\n",
      "(40, 1.2094162)\n",
      "(21, 1.1958565)\n",
      "(26, 1.0776446)\n",
      "(4, -1.0232619)\n",
      "(63, -1.0156202)\n"
     ]
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 63838, \"xhqqevmyaua\", False, False)\n",
    "print(idx[:32])\n",
    "for i in range(32):\n",
    "    print(idx[i], grads.reshape((MAX_FILE_SIZE,))[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  51,   9,   0,  58,  55,  19,  56,  45,   7,   1,   3,   5,\n",
       "        53,  33,  44,  60,  15,  20,  46,   6,  21,  34,  32,  43,  54,\n",
       "        50,  31,  35,  30,  42,  13,  16,  10,  57,   8,  14,  12,  24,\n",
       "        26,  18,  41,  28,  49,  22,   4,  48,  17,  29,  59,  37,  38,\n",
       "       338,  25, 444,  23, 390,  52,  63,  27, 223, 106, 475, 283, 103,\n",
       "       389, 502, 122, 367, 466, 197, 221, 321, 494, 453, 395, 400,  75,\n",
       "       378, 264,  61, 270, 414, 438, 342, 361, 246, 211,  78, 462,  72,\n",
       "       316, 300, 447, 105, 377, 107, 267, 275, 192, 302, 174, 265, 249,\n",
       "       402,  39, 252, 433, 263, 473, 169, 280, 156, 261, 172, 206, 164,\n",
       "       124, 474, 346,  76, 157, 142, 306,  90, 196, 307, 385, 391,  74,\n",
       "       345, 114, 360, 282, 369, 274, 384, 150, 297,  62, 355, 286, 320,\n",
       "       133, 417, 352,  73,  84, 115, 244, 100, 335, 230, 424, 238, 173,\n",
       "       226, 248, 199, 117, 193, 140, 451, 381, 147, 362, 205, 380, 498,\n",
       "       309, 442, 212, 276, 358, 120, 102, 121, 153, 333, 490, 152, 227,\n",
       "        64, 171,  97, 471, 480, 296, 258, 408, 235, 459, 285, 509,  65,\n",
       "        94, 312, 207, 503, 125, 161,  83, 269, 435,  68, 111, 127, 217,\n",
       "       141, 317, 237, 457,  47, 303, 101, 436, 482, 318, 392, 262, 308,\n",
       "       500,  66, 411, 331, 437, 165, 373, 290, 505, 448, 348, 134, 236,\n",
       "       305, 388, 431, 183, 187, 234,  77, 281, 372, 277, 423, 404, 376,\n",
       "       463, 128, 218, 184, 401, 322, 233, 364, 129, 224, 366, 146, 359,\n",
       "       481, 332, 341, 470, 510, 334, 139, 170,  70, 504, 416, 472, 195,\n",
       "       499,  36, 374, 255, 204, 354,  98, 210, 419, 427, 228, 353, 145,\n",
       "       351, 452, 493, 293, 497, 511, 314, 299, 186, 508, 162, 242, 271,\n",
       "       198, 119, 289, 154, 177, 458, 304, 418, 487, 347, 339, 440, 421,\n",
       "       382, 496, 257, 432, 420, 189, 163, 243, 484, 467, 180, 214, 326,\n",
       "       479, 216, 176, 406,  85, 410, 344, 104, 349, 455, 130, 357, 368,\n",
       "        89, 485,  93, 397, 175, 469, 311, 132, 194, 143, 167, 110, 188,\n",
       "       215, 428, 137, 426, 403, 430, 412, 429, 386, 241, 315, 398,  81,\n",
       "       273, 464, 272, 350,  79, 491, 379, 450, 415,  40,  96, 294, 278,\n",
       "       328, 123, 144, 478, 247,  69, 287, 507, 393,  95, 219, 375, 116,\n",
       "       489, 413, 365, 213, 313, 399, 483, 336, 501, 409, 292, 148, 454,\n",
       "       356, 239, 343, 319, 191, 113, 200, 112, 387, 109, 476, 136, 329,\n",
       "        99, 250, 327,  88, 279, 425,  71, 383, 222, 168, 245, 340, 443,\n",
       "       298, 446, 203, 151,  87, 422, 178, 330, 288, 371, 266, 465, 337,\n",
       "       407, 291, 159, 370, 259, 202,  80,  82, 363, 158, 208, 118, 254,\n",
       "       323, 185, 284, 201, 405, 209, 179, 260, 439, 396, 225, 486, 240,\n",
       "       495, 434, 135, 220, 449, 461, 506, 138, 325, 232, 324, 182, 460,\n",
       "        91, 394, 468, 190, 445,  67,  92, 477, 126, 253, 251, 301,  86,\n",
       "       131, 166, 181, 231, 268, 488, 492, 441, 160, 456, 295, 310, 256,\n",
       "       229, 108, 149, 155,  11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, val, grads = gradient(model, 46697, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", True, False)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, -0.9926252)\n",
      "(51, 0.9877852)\n",
      "(9, -0.94090235)\n",
      "(0, -0.9284056)\n",
      "(58, 0.9225007)\n",
      "(55, 0.9210491)\n",
      "(19, -0.8444284)\n",
      "(56, 0.78428847)\n",
      "(45, 0.7777022)\n",
      "(7, -0.76277465)\n",
      "(1, -0.59241474)\n",
      "(3, -0.519421)\n",
      "(5, -0.49340814)\n",
      "(53, 0.4735327)\n",
      "(33, -0.4687665)\n",
      "(44, 0.43126237)\n",
      "(60, 0.43033266)\n",
      "(15, -0.41055226)\n",
      "(20, -0.40064228)\n",
      "(46, 0.39519095)\n",
      "(6, -0.38543844)\n",
      "(21, -0.35823664)\n",
      "(34, -0.35743427)\n",
      "(32, -0.35670954)\n",
      "(43, -0.35147437)\n",
      "(54, 0.3381382)\n",
      "(50, 0.3327644)\n",
      "(31, 0.31581885)\n",
      "(35, -0.31480682)\n",
      "(30, 0.3088276)\n",
      "(42, 0.30355877)\n",
      "(13, -0.29671925)\n",
      "(16, -0.28662738)\n",
      "(10, -0.27887574)\n",
      "(57, 0.27090114)\n",
      "(8, 0.25896052)\n",
      "(14, -0.25012976)\n",
      "(12, -0.24485919)\n",
      "(24, -0.2355277)\n",
      "(26, -0.23170716)\n",
      "(18, -0.21151908)\n",
      "(41, -0.20655754)\n",
      "(28, 0.18687436)\n",
      "(49, -0.1807828)\n",
      "(22, -0.16419214)\n",
      "(4, -0.15219514)\n",
      "(48, 0.13927597)\n",
      "(17, -0.13549107)\n",
      "(29, 0.13072295)\n",
      "(59, -0.11105962)\n",
      "(37, -0.10061196)\n",
      "(38, 0.09978367)\n",
      "(338, -0.093771756)\n",
      "(25, -0.089932725)\n",
      "(444, -0.089920774)\n",
      "(23, 0.081211895)\n",
      "(390, 0.07882668)\n",
      "(52, 0.07639997)\n",
      "(63, -0.068838485)\n",
      "(27, 0.0665694)\n",
      "(223, 0.064742036)\n",
      "(106, 0.060564972)\n",
      "(475, 0.059700835)\n",
      "(283, -0.05921168)\n",
      "(103, -0.058053583)\n",
      "(389, -0.05600343)\n",
      "(502, -0.054901607)\n",
      "(122, 0.053938005)\n",
      "(367, 0.052974835)\n",
      "(466, 0.05257652)\n",
      "(197, 0.052201197)\n",
      "(221, -0.051925074)\n",
      "(321, -0.051593043)\n",
      "(494, 0.05124317)\n",
      "(453, -0.051035307)\n",
      "(395, -0.050892934)\n",
      "(400, 0.050480496)\n",
      "(75, -0.048433818)\n",
      "(378, 0.048034664)\n",
      "(264, 0.048019063)\n",
      "(61, 0.047983743)\n",
      "(270, 0.047016166)\n",
      "(414, 0.04635376)\n",
      "(438, -0.046171412)\n",
      "(342, -0.045913152)\n",
      "(361, -0.045858137)\n",
      "(246, -0.045181043)\n",
      "(211, -0.045016453)\n",
      "(78, 0.04500331)\n",
      "(462, 0.04424995)\n",
      "(72, -0.043549873)\n",
      "(316, -0.043163)\n",
      "(300, -0.043147665)\n",
      "(447, -0.043090157)\n",
      "(105, -0.043067522)\n",
      "(377, 0.04295776)\n",
      "(107, 0.04247427)\n",
      "(267, -0.042436205)\n",
      "(275, 0.042197477)\n",
      "(192, -0.042039007)\n",
      "(302, -0.041504424)\n",
      "(174, -0.041455247)\n",
      "(265, -0.04133141)\n",
      "(249, -0.041233603)\n",
      "(402, -0.041095756)\n",
      "(39, -0.040780906)\n",
      "(252, -0.040762026)\n",
      "(433, 0.040617645)\n",
      "(263, 0.04030273)\n",
      "(473, -0.0401375)\n",
      "(169, -0.03979087)\n",
      "(280, -0.03977707)\n",
      "(156, 0.03973327)\n",
      "(261, -0.039467126)\n",
      "(172, 0.039063156)\n",
      "(206, -0.038700353)\n",
      "(164, 0.038671747)\n",
      "(124, -0.038592204)\n",
      "(474, -0.038528554)\n",
      "(346, -0.038436506)\n",
      "(76, -0.037966967)\n",
      "(157, -0.037834417)\n",
      "(142, 0.037755694)\n",
      "(306, 0.037540726)\n",
      "(90, -0.03727585)\n",
      "(196, -0.037169866)\n",
      "(307, 0.03701698)\n",
      "(385, 0.036848135)\n",
      "(391, 0.0367118)\n",
      "(74, 0.0366337)\n",
      "(345, -0.036624104)\n",
      "(114, 0.036195077)\n",
      "(360, -0.035835076)\n",
      "(282, 0.035535567)\n",
      "(369, -0.03529803)\n",
      "(274, 0.035019375)\n",
      "(384, -0.034729406)\n",
      "(150, -0.03461051)\n",
      "(297, -0.034094673)\n",
      "(62, -0.034050137)\n",
      "(355, -0.033863712)\n",
      "(286, 0.033822108)\n",
      "(320, 0.03382048)\n",
      "(133, 0.0337845)\n",
      "(417, -0.033518948)\n",
      "(352, 0.032893226)\n",
      "(73, 0.032866787)\n",
      "(84, -0.03285508)\n",
      "(115, 0.032769747)\n",
      "(244, 0.03273321)\n",
      "(100, 0.0324879)\n",
      "(335, 0.031810388)\n",
      "(230, 0.03151886)\n",
      "(424, 0.031499088)\n",
      "(238, -0.0313309)\n",
      "(173, -0.03129307)\n",
      "(226, -0.031150002)\n",
      "(248, -0.031130817)\n",
      "(199, 0.03112096)\n",
      "(117, 0.030859126)\n",
      "(193, -0.030718368)\n",
      "(140, 0.030599816)\n",
      "(451, 0.030556004)\n",
      "(381, 0.03049868)\n",
      "(147, 0.030055126)\n",
      "(362, 0.030049853)\n",
      "(205, -0.029858299)\n",
      "(380, 0.02964161)\n",
      "(498, 0.029636778)\n",
      "(309, 0.029569838)\n",
      "(442, -0.029498082)\n",
      "(212, -0.02929026)\n",
      "(276, 0.029150186)\n",
      "(358, -0.028934874)\n",
      "(120, -0.028910685)\n",
      "(102, -0.028880432)\n",
      "(121, -0.028845984)\n",
      "(153, 0.028729882)\n",
      "(333, 0.028701479)\n",
      "(490, -0.028670913)\n",
      "(152, -0.028408289)\n",
      "(227, -0.02835019)\n",
      "(64, -0.028217798)\n",
      "(171, 0.028210048)\n",
      "(97, 0.027883962)\n",
      "(471, -0.027818158)\n",
      "(480, -0.027813908)\n",
      "(296, -0.027704168)\n",
      "(258, -0.027610134)\n",
      "(408, 0.02756556)\n",
      "(235, 0.027252315)\n",
      "(459, 0.027162522)\n",
      "(285, 0.027092254)\n",
      "(509, 0.027034987)\n",
      "(65, 0.027017787)\n",
      "(94, 0.026991326)\n",
      "(312, 0.0268686)\n",
      "(207, 0.026815461)\n",
      "(503, -0.026785662)\n",
      "(125, -0.02677073)\n",
      "(161, 0.026644763)\n",
      "(83, -0.026538553)\n",
      "(269, 0.026362665)\n",
      "(435, 0.026279088)\n",
      "(68, -0.026161548)\n",
      "(111, 0.026042622)\n",
      "(127, 0.025900222)\n",
      "(217, 0.025793731)\n",
      "(141, 0.025667094)\n",
      "(317, 0.0255703)\n",
      "(237, -0.025514323)\n",
      "(457, -0.025509045)\n",
      "(47, -0.025433242)\n",
      "(303, -0.025410186)\n",
      "(101, 0.025387358)\n",
      "(436, 0.025171276)\n",
      "(482, -0.025131177)\n",
      "(318, 0.025020245)\n",
      "(392, 0.024903424)\n",
      "(262, -0.024834817)\n",
      "(308, 0.024802282)\n",
      "(500, 0.024711626)\n",
      "(66, 0.024472695)\n",
      "(411, 0.024463886)\n",
      "(331, 0.024335658)\n",
      "(437, -0.024135515)\n",
      "(165, -0.023947217)\n",
      "(373, -0.0237757)\n",
      "(290, -0.023647547)\n",
      "(505, 0.02358539)\n",
      "(448, 0.023565583)\n",
      "(348, 0.02342797)\n",
      "(134, -0.023423541)\n",
      "(236, 0.023246618)\n",
      "(305, -0.023245202)\n",
      "(388, -0.023217015)\n",
      "(431, 0.023208857)\n",
      "(183, -0.023065226)\n",
      "(187, -0.023027364)\n",
      "(234, -0.02283892)\n",
      "(77, 0.022803716)\n",
      "(281, -0.022796351)\n",
      "(372, 0.022767212)\n",
      "(277, 0.022638764)\n",
      "(423, -0.022576572)\n",
      "(404, 0.022485828)\n",
      "(376, 0.022478644)\n",
      "(463, -0.022412967)\n",
      "(128, -0.022396302)\n",
      "(218, 0.022234216)\n",
      "(184, 0.02211568)\n",
      "(401, 0.022069164)\n",
      "(322, 0.022029176)\n",
      "(233, -0.021914188)\n",
      "(364, 0.021862157)\n",
      "(129, -0.02184213)\n",
      "(224, -0.021829352)\n",
      "(366, -0.021787258)\n",
      "(146, 0.021772359)\n",
      "(359, -0.021507815)\n",
      "(481, 0.021236043)\n",
      "(332, 0.02120127)\n",
      "(341, 0.021166736)\n",
      "(470, -0.020910308)\n",
      "(510, 0.020784348)\n",
      "(334, 0.020780344)\n",
      "(139, 0.02073013)\n",
      "(170, 0.02072351)\n",
      "(70, -0.020700142)\n",
      "(504, 0.020695832)\n",
      "(416, 0.020681653)\n",
      "(472, -0.020587914)\n",
      "(195, 0.02044914)\n",
      "(499, -0.020381467)\n",
      "(36, -0.020378271)\n",
      "(374, 0.02037272)\n",
      "(255, 0.02023165)\n",
      "(204, -0.02014505)\n",
      "(354, -0.020134624)\n",
      "(98, 0.01995291)\n",
      "(210, -0.01987444)\n",
      "(419, 0.019780705)\n",
      "(427, -0.019779133)\n",
      "(228, -0.01946203)\n",
      "(353, 0.019425536)\n",
      "(145, 0.019206328)\n",
      "(351, 0.019171633)\n",
      "(452, -0.019114817)\n",
      "(493, 0.018983798)\n",
      "(293, -0.01882451)\n",
      "(497, -0.018679567)\n",
      "(511, 0.018663712)\n",
      "(314, -0.018485708)\n",
      "(299, -0.018230628)\n",
      "(186, -0.018187776)\n",
      "(508, -0.01809131)\n",
      "(162, 0.017738298)\n",
      "(242, 0.017640417)\n",
      "(271, 0.017519023)\n",
      "(198, -0.017373366)\n",
      "(119, -0.017330592)\n",
      "(289, 0.017303482)\n",
      "(154, -0.017285207)\n",
      "(177, 0.017284207)\n",
      "(458, -0.017262124)\n",
      "(304, 0.017117448)\n",
      "(418, 0.016944095)\n",
      "(487, 0.016770303)\n",
      "(347, 0.016737007)\n",
      "(339, 0.016735207)\n",
      "(440, -0.016647922)\n",
      "(421, 0.01661002)\n",
      "(382, 0.016574636)\n",
      "(496, -0.01643581)\n",
      "(257, -0.016322508)\n",
      "(432, -0.016264033)\n",
      "(420, -0.016212665)\n",
      "(189, 0.016197221)\n",
      "(163, 0.0161959)\n",
      "(243, -0.016135274)\n",
      "(484, 0.016100096)\n",
      "(467, 0.01609149)\n",
      "(180, 0.015928993)\n",
      "(214, 0.015777238)\n",
      "(326, -0.015347859)\n",
      "(479, -0.015335945)\n",
      "(216, 0.015027985)\n",
      "(176, -0.015014564)\n",
      "(406, -0.014998797)\n",
      "(85, 0.0148130115)\n",
      "(410, -0.014711045)\n",
      "(344, -0.014593672)\n",
      "(104, 0.014432884)\n",
      "(349, -0.014375217)\n",
      "(455, -0.0142833125)\n",
      "(130, -0.014193773)\n",
      "(357, -0.014158907)\n",
      "(368, 0.014106454)\n",
      "(89, -0.013822464)\n",
      "(485, 0.013820922)\n",
      "(93, 0.013532048)\n",
      "(397, -0.013491841)\n",
      "(175, -0.01341031)\n",
      "(469, 0.013281485)\n",
      "(311, 0.013188137)\n",
      "(132, -0.013061041)\n",
      "(194, -0.013058903)\n",
      "(143, -0.013034364)\n",
      "(167, 0.012929777)\n",
      "(110, -0.012836621)\n",
      "(188, 0.012740329)\n",
      "(215, 0.01253575)\n",
      "(428, 0.012532786)\n",
      "(137, -0.012530823)\n",
      "(426, 0.012268413)\n",
      "(403, 0.012239814)\n",
      "(430, 0.01222719)\n",
      "(412, 0.012225078)\n",
      "(429, -0.012215961)\n",
      "(386, -0.011842319)\n",
      "(241, 0.011698332)\n",
      "(315, 0.011399165)\n",
      "(398, -0.011227975)\n",
      "(81, -0.0111141)\n",
      "(273, 0.011061883)\n",
      "(464, -0.0110539375)\n",
      "(272, 0.010996452)\n",
      "(350, 0.01097887)\n",
      "(79, 0.010785101)\n",
      "(491, -0.010695154)\n",
      "(379, -0.010665538)\n",
      "(450, -0.010496269)\n",
      "(415, -0.010344848)\n",
      "(40, -0.010091109)\n",
      "(96, 0.009966714)\n",
      "(294, -0.00990813)\n",
      "(278, 0.009906269)\n",
      "(328, 0.009864606)\n",
      "(123, 0.009800276)\n",
      "(144, 0.0097155)\n",
      "(478, -0.00954217)\n",
      "(247, -0.009533059)\n",
      "(69, 0.00940337)\n",
      "(287, -0.009398043)\n",
      "(507, 0.00925573)\n",
      "(393, -0.009211924)\n",
      "(95, 0.009196193)\n",
      "(219, -0.009132473)\n",
      "(375, 0.009108282)\n",
      "(116, 0.008821569)\n",
      "(489, 0.0087733455)\n",
      "(413, -0.008679682)\n",
      "(365, -0.008614501)\n",
      "(213, 0.008581267)\n",
      "(313, -0.008570921)\n",
      "(399, -0.008460989)\n",
      "(483, 0.008341059)\n",
      "(336, 0.008300158)\n",
      "(501, 0.008215848)\n",
      "(409, -0.008214216)\n",
      "(292, -0.008114431)\n",
      "(148, 0.008111465)\n",
      "(454, 0.008057559)\n",
      "(356, -0.008022146)\n",
      "(239, 0.008010922)\n",
      "(343, 0.0079898)\n",
      "(319, -0.007964463)\n",
      "(191, -0.0078365505)\n",
      "(113, 0.0077918777)\n",
      "(200, -0.0075567784)\n",
      "(112, 0.0074556638)\n",
      "(387, -0.007405891)\n",
      "(109, -0.0073248427)\n",
      "(476, 0.0072716493)\n",
      "(136, 0.007186237)\n",
      "(329, -0.007118352)\n",
      "(99, 0.0071133235)\n",
      "(250, 0.0070828106)\n",
      "(327, -0.0070095286)\n",
      "(88, 0.0069526155)\n",
      "(279, -0.006948482)\n",
      "(425, -0.0068522384)\n",
      "(71, -0.0067912005)\n",
      "(383, 0.0067722015)\n",
      "(222, 0.0067461506)\n",
      "(168, -0.006705967)\n",
      "(245, -0.0066462737)\n",
      "(340, -0.006566448)\n",
      "(443, -0.006558029)\n",
      "(298, 0.0064437315)\n",
      "(446, 0.0062072705)\n",
      "(203, -0.0061372146)\n",
      "(151, 0.0060850307)\n",
      "(87, 0.0059832223)\n",
      "(422, -0.0059632743)\n",
      "(178, 0.005944746)\n",
      "(330, 0.0059071593)\n",
      "(288, 0.0058792382)\n",
      "(371, 0.0058621997)\n",
      "(266, -0.0057725105)\n",
      "(465, -0.005710203)\n",
      "(337, 0.0056114257)\n",
      "(407, -0.005521018)\n",
      "(291, 0.0054535735)\n",
      "(159, -0.005416483)\n",
      "(370, -0.0053112656)\n",
      "(259, -0.005172558)\n",
      "(202, -0.0051270677)\n",
      "(80, 0.0047356114)\n",
      "(82, 0.004622935)\n",
      "(363, -0.004617133)\n",
      "(158, -0.0045695826)\n",
      "(208, 0.004393043)\n",
      "(118, 0.0043368274)\n",
      "(254, -0.004242379)\n",
      "(323, -0.0041704047)\n",
      "(185, -0.0041674655)\n",
      "(284, 0.0041419766)\n",
      "(201, -0.004044078)\n",
      "(405, 0.003956912)\n",
      "(209, -0.0037792446)\n",
      "(179, -0.0036471188)\n",
      "(260, -0.0035797358)\n",
      "(439, -0.003547063)\n",
      "(396, -0.0034336513)\n",
      "(225, 0.003430374)\n",
      "(486, 0.0033800416)\n",
      "(240, -0.0032963983)\n",
      "(495, 0.0032631978)\n",
      "(434, -0.003253323)\n",
      "(135, -0.0030742465)\n",
      "(220, -0.003043157)\n",
      "(449, -0.003035171)\n",
      "(461, 0.0029371977)\n",
      "(506, 0.0029359767)\n",
      "(138, -0.0029198844)\n",
      "(325, -0.0027781767)\n",
      "(232, 0.002705378)\n",
      "(324, -0.0026627025)\n",
      "(182, -0.0025660861)\n",
      "(460, 0.0024568774)\n",
      "(91, 0.0023786505)\n",
      "(394, 0.0023684995)\n",
      "(468, 0.0022662366)\n",
      "(190, 0.0021965094)\n",
      "(445, 0.001933869)\n",
      "(67, -0.0019234763)\n",
      "(92, -0.0018901415)\n",
      "(477, -0.0016440675)\n",
      "(126, 0.001498892)\n",
      "(253, 0.001326696)\n",
      "(251, 0.001314233)\n",
      "(301, 0.0013119737)\n",
      "(86, -0.0011720695)\n",
      "(131, 0.001140357)\n",
      "(166, 0.001106536)\n",
      "(181, 0.0009770552)\n",
      "(231, -0.0008877788)\n",
      "(268, 0.0008383244)\n",
      "(488, 0.0007426208)\n",
      "(492, -0.0006468743)\n",
      "(441, -0.00057533616)\n",
      "(160, 0.00051479274)\n",
      "(456, -0.00050393865)\n",
      "(295, 0.00042629708)\n",
      "(310, -0.00035921484)\n",
      "(256, 0.00032586884)\n",
      "(229, 0.00031127222)\n",
      "(108, -0.0003107153)\n",
      "(149, 0.00029914826)\n",
      "(155, 0.00016779453)\n",
      "(11, -9.069219e-05)\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_FILE_SIZE):\n",
    "    print(idx[i], grads.reshape((MAX_FILE_SIZE,))[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 51, 56, 55, 45, 60,  9,  0, 19, 33,  2, 53, 44, 59, 46, 43,  7,\n",
       "       34, 42, 32, 20, 35, 50, 26,  1,  5, 49, 24, 21, 57, 41, 54])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, val = gradient(model, 46697, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", True, False)\n",
    "idx[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, seed, vectorize=False):\n",
    "    x = vectorize_file(seed, True, vectorize)\n",
    "    file_name = file_name = \"./bitmaps/\"+ seed.split('/')[-1] + \".npy\"\n",
    "    return model.predict(x), np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ty = evaluate(model, \"neuzz_in/id:000134,src:000131,op:int8,pos:63,val:+0,+cov\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.360158e-32,\n",
       "        0.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00,\n",
       "        1.000000e+00, 1.000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuzz",
   "language": "python",
   "name": "neuzz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
